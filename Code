import numpy as np
import pandas as pd
from scipy.fftpack import fft
from scipy.stats import entropy
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'X' as features and 'y' as labels
# Example: X, y = data.drop('label', axis=1), data['label']


# *1. Label Encoding for Target Variable*
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  
label_mapping = {label: idx for idx, label in enumerate(label_encoder.classes_)}
print("Label Encoding Mapping:", label_mapping)


# *2. Advanced Feature Extraction with FFT, Energy, and Entropy*
def extract_frequency_features(signal):
    fft_values = np.abs(fft(signal))  
    total_energy = np.sum(fft_values ** 2)  
    signal_entropy = entropy(fft_values)  
    dominant_frequency = np.argmax(fft_values)  
    return total_energy, signal_entropy, dominant_frequency

# Apply feature extraction to each row in X
frequency_features = np.array([extract_frequency_features(row) for row in X.values])
freq_df = pd.DataFrame(frequency_features, columns=['total_energy', 'signal_entropy', 'dominant_frequency'])
X = pd.concat([X, freq_df], axis=1)

# *3. Feature Scaling and Selection*
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Select the top k best features using SelectKBest
k_best = min(7, X.shape[1])  # Adjust k dynamically
selector = SelectKBest(score_func=f_classif, k=k_best)
X_selected = selector.fit_transform(X_scaled, y_encoded)
selected_features = X.columns[selector.get_support(indices=True)]
print("Selected Features:", selected_features)

# *4. Random Forest Training and Hyperparameter Tuning*
param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [5, 10],
    'min_samples_split': [2, 5]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42), param_distributions=param_grid, scoring='f1_weighted',
    cv=5, n_iter=5, n_jobs=-1, random_state=42, verbose=1
)
random_search.fit(X_selected, y_encoded)
best_rf_clf = random_search.best_estimator_

# *5. Final Model Evaluation*
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42)

# Train the model and predict
best_rf_clf.fit(X_train, y_train)
y_pred = best_rf_clf.predict(X_test)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Calculate F1 Score
f1 = f1_score(y_test, y_pred, average='weighted')
print("F1 Score:", f1)

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix for Random Forest")
plt.show()
